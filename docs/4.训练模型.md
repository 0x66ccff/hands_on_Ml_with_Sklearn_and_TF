# 第四章:训练模型

在之前的描述中，我们通常把机器学习模型和训练算法当作黑箱子来处理。如果你实践过前几章的一些示例，你惊奇的发现你可以优化回归系统，改进数字图像的分类器，你甚至可以零基础搭建一个垃圾邮件的分类器，但是你却对它们内部的工作流程一无所知。事实上，许多场合你都不需要知道这些黑箱子的内部有什么，干了什么。

然而，如果你对其内部的工作流程有一定了解的话，当面对一个机器学习任务时候，这些理论可以帮助你快速的找到恰当的机器学习模型，合适的训练算法，以及一个好的假设集。同时，了解黑箱子内部的构成，有助于你更好地调试参数以及更有效的误差分析。本章讨论的大部分话题对于机器学习模型的理解，构建，以及神经网络（详细参考本书的第二部分）的训练都是非常重要的。

首先我们将以一个简单的线性回归模型为例，讨论两种不同的训练方法来得到模型的最优解：

+ 直接使用封闭方程进行求根运算，得到模型在当前训练集上的最优参数（即在训练集上使损失函数达到最小值的模型参数）

+ 使用迭代优化方法：梯度下降（GD），在训练集上，它可以逐渐调整模型参数以获得最小的损失函数，最终，参数会收敛到和第一种方法相同的的值。同时，我们也会介绍一些梯度下降的变体形式：批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD），在第二部分的神经网络部分，我们会多次使用它们。

接下来，我们将研究一个更复杂的模型：多项式回归，它可以拟合非线性数据集，由于它比线性模型拥有更多的参数，于是它更容易出现模型的过拟合。因此，我们将介绍如何通过学习曲线去判断模型是否出现了过拟合，并介绍几种正则化方法以减少模型出现过拟合的风险。

最后，我们将介绍两个常用于分类的模型：Logistic回归和Softmax回归

**提示：** *在本章中包含许多数学公式，以及一些线性代数和微积分基本概念。为了理解这些公式，你需要知道什么是向量，什么是矩阵，以及它们直接是如何转化的，以及什么是点积，什么是矩阵的逆，什么是偏导数。如果你对这些不是很熟悉的话，你可以阅读本书提供的 Jupyter在线笔记，它包括了线性代数和微积分的入门指导。对于那些不喜欢数学的人，你也应该快速简单的浏览这些公式。希望它足以帮助你理解大多数的概念。*

## 线性回归

在第一章，我们介绍了一个简单的的生活满意度回归模型:

>life_satisfaction = $\theta _{0} $ + $\theta _{1}$ * GDP_per_capita

这个模型仅仅是输入量GDP_per_capita的线性函数，$\theta _{0} $和$\theta _{1}$是这个模型的参数，线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。如公式4-1：

> 公式4-1：线性回归预测模型
> $\hat{y} = \theta _{0} + \theta _{1}x _{1}+\theta _{2}x _{2}+\dots+\theta _{n}x _{n}$
>
> + $\hat{y}$表示预测结果
> + $n$表示特征的个数
> + $x _{i}$表示第i个特征的值
> + $\theta_{j}$表示第j个参数（包括偏置项$\theta _{0}$和特征权重值$\theta _{1},\theta _{2},\dots,\theta _{n}$）

上述公式可以写成更为简洁的向量形式，如公式4-2：

> 公式4-2：线性回归预测模型（向量形式）
> $\hat{y} = h _{\theta} (\mathbf{x})= \theta^T  \cdot \mathbf{x}$
>
> + $\theta$表示模型的参数向量包括偏置项$\theta _{0}$和特征权重值$\theta _{1}$到$\theta _{n}$
> + $\theta^T$表示向量$\theta$的转置（行向量变为了列向量）
> + $\mathbf{x}$为每个样例中特征值的向量形式，包括$x _{1}$到$x_{n}$，而且$x_0$恒为1
> + $\theta^T  \cdot \mathbf{x}$表示$\theta^T$和$ \mathbf{x}$的点积
> + $h_{\theta}$表示参数为$\theta$的假设函数

怎么样去训练一个线性回归模型呢？好吧，回想一下，训练一个模型指的是设置模型的参数使得这个模型在训练集的表现较好。为此，我们首先需要找到一个衡量模型好坏的评定方法。在第二章，我们介绍到在回归模型上，最常见的评定标准是均方根误差（RMSE,详见公式2-1）。因此，为了训练一个线性回归模型，你需要找到一个$\theta$值，它使得均方根误差（标准误差）达到最小值。实践过程中，最小化均方误差比最小化均方根误差更加的简单，这两个过程会得到相同的$\theta$,因为函数在最小值时候的自变量，同样能使函数的方根运算得到最小值。

在训练集$\mathbf{X}$上使用公式4-3来计算线性回归假设$h_{\theta}$的均方差（$MSE$）。

> 公式4-3：线性回归模型的MES代价函数
> $MSE (\mathbf{X},h _{\theta}$) = $\frac{1}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}^2$

公式中符号的含义大多数都在第二章（详见“符号”）进行了说明，不同的是：为了突出模型的参数向量$\theta$,使用$h_{\theta}$来代替$h$。以后的使用中为了公式的简洁，使用$MSE(\theta)$来代替$MSE (\mathbf{X},h _{\theta}$) 。

### 正规方程

为了找到最小化代价函数的$\theta$值，可以采用公式解，换句话说，就是可以通过解正规方程直接得到最后的结果。

> 公式4-4：正规方程
> $\hat{\theta} = ({\mathbf{X}}^T\cdot\mathbf{X})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}$
>
> + $\hat{\theta}$指最小化代价$的值
> + $\mathbf{y}$是一个向量，其包含了$y^{(1)}$到$y^{(m)}$的值

让我们生成一些近似线性的数据（如图4-1）来测试一下这个方程。

```python
import numpy as np 
X = 2 * np.random.rand(100,1)
y = 4 + 3 * X + np.random.randn(100,1)
```

![](C:\Users\king\Documents\GitHub\hands_on_Ml_with_Sklearn_and_TF\images\chapter_4\图4-1.PNG)
                                      图4-1:随机线性数据集

现在让我们使用正规方程来计算$\hat{\theta}$，我们将使用Numpy的线性代数模块(np.linalg)中的`inv()`函数来计算矩阵的逆，以及`dot()`方法来计算矩阵的乘法。	

```python 
X_b = np.c_[np.ones((100, 1)), X] 
theta_best = np.linalg.inv(X_b.T.dot(X_B)).dot(X_b.T).dot(y)
```
我们生产数据的函数实际上是$y = 4 + 3x_0 + 高斯噪声$。让我们看一下最后的计算结果。
```python 
>>> theta_best
array([[4.21509616],[2.77011339]])
```
我们希望最后得到的参数$\theta_0=4,\theta_1=3$而不是$\theta_0=3.865,\theta_1=3.139$(译者注：我认为应该是$\theta_0=4.2150,\theta_1=2.7701$)。这已经足够了，由于存在噪声，参数不可能达到到原始函数的值。

现在我们能够使用$\hat{\theta}$来进行预测：

```python 
>>> X_new = np.array([[0],[2]])
>>> X_new_b = np.c_[np.ones((2, 1)), X_new]
>>> y_predict = X_new_b.dot(theta.best)
>>> y_predict
array([[4.21509616],[9.75532293]])
```
画出这个模型的图像，如图4-2
```python 
plt.plot(X_new,y_predict,"r-")
plt.plot(X,y,"b.")
plt.axis([0,2,0,15])
plt.show()
```
![](C:\Users\king\Documents\GitHub\hands_on_Ml_with_Sklearn_and_TF\images\chapter_4\图4-2.PNG)
                                      图4-2：线性回归预测	

使用下面的Scikit-Learn代码可以达到相同的效果：

```python
>>> form sklearn.linear_model import LinearRegression
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X,y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([4.21509616]),array([2.77011339]))
>>> lin_reg.predict(X_new)
array([[4.21509616],[9.75532293]])
```

### 计算复杂度
正规化方程需要计算矩阵${\mathbf{X}}^T\cdot\mathbf{X}$的逆，它是一个$n * n$的矩阵（$n$是特征的个数）。这样一个矩阵求逆的运算复杂度大约在$O(n^{2.4})$到$O(n^3)$之间，具体值取决于计算方式。换句话说，如果你将你的特征个数翻倍的话，其计算时间大概会变为原来的5.3（$2^{2.4}$）$到$8（$2^3$）倍。

**提示：** *当特征的个数较大的时候（例如：特征数量为100000），正规方程求解将会非常慢.。*

有利的一面是，这个方程在训练集上对于每一个实例来说是线性的，其复杂度为$O(m)$，男因此只要有能放得下它的内存空间，它就可以对大规模数据的进行训练。同时，一旦你得到了线性回归模型（通过解正规方程或者其他的算法），进行预测是非常快的。因为模型中计算复杂度对于要进行预测的实例个数量和特征个数都是线性的。 换句话说，当实例个数变为原来的两倍多的时候（或特征个数变为原来的两倍多），预测时间也仅仅是原来的两倍多。

接下来，我们将介绍另一种方法去训练模型。这种方法适合在特征个数非常多，训练实例非常多，内存无法满足要求的时候使用。

## 梯度下降

梯度下降是一种非常通用的优化算法，它能够很好地解决一系列问题。梯度下降的整体思路是通过的迭代来逐渐调整参数使得代价函数达到最小值。

假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量$\theta$的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。

具体来说，开始时，需要选定一个随机的$\theta$（这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低代价函数（例如：均方差代价函数），直到算法收敛到一个最小值（如图：4-3）。

![](C:\Users\king\Documents\GitHub\hands_on_Ml_with_Sklearn_and_TF\images\chapter_4\图4-3.PNG)
                                      图4-3：梯度下降

在梯度下降中一个重要的参数是每一步的大小，超参数学习率决定了其大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的（如图4-4）。
![](C:\Users\king\Documents\GitHub\hands_on_Ml_with_Sklearn_and_TF\images\chapter_4\图4-4.PNG)
                                      图4-4:学习率太小

另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案（如图4-5）。
![](C:\Users\king\Documents\GitHub\hands_on_Ml_with_Sklearn_and_TF\images\chapter_4\图4-5.PNG)
                                      图4-5：学习率太大

最后，并不是所有的代价函数看起来都像一个规则的碗。它们可能是洞，山脊，高原和各种不规则的地形，使它们收敛到最小值非常的困难。 图4-6显示了梯度下降的两个主要挑战：如果随机初始值选在了图像的左侧，则它将收敛到局部最小值，这个值要比全局最小值要大。 如果它从右侧开始，那么跨越高原将需要很长时间，如果你早早的结束训练，你将永远到不了全局最小值。

![](C:\Users\king\Documents\GitHub\hands_on_Ml_with_Sklearn_and_TF\images\chapter_4\图4-6.PNG)
                                      图4-6：梯度下降的陷阱
## 多项式回归

## 学习曲线

## 线性模型的正则化

### 岭回归(Ridge)

### Lasso回归(Lasso)

### 弹性回归(ElasticNet)

### 早期停止法

## 逻辑回归

## 练习



