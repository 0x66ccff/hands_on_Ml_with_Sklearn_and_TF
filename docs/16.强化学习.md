# 第16章 强化学习

强化学习(RL)如今是机器学习的一大令人激动的领域，当然之前也是。自从1950年被发明出来后，它在这些年产生了一些有趣的应用，尤其是在游戏(例如TD-Gammon，一个西洋双陆棋程序)和及其控制领域。，但是从未弄出什么大新闻。直到2013年一个革命性的发展：来自英国的研究者发起了一项Deepmind项目，这个项目可以学习去玩任何从头开始的Atari游戏，甚至多数比人类玩的还要好，它仅适用像素作为输入并且没有游戏规则的任何先验知识。这是一系列令人惊叹的壮举，在2016年3月以他们的系统阿尔法狗战胜了世界围棋冠军李世石。没有一个程序能接近这个游戏的主宰，更不用说世界冠军了。今天，RL的整个领域正在沸腾着新的想法，其都具有广泛的应用范围。DeepMind在2014被谷歌以超过5亿美元收购。

那么他们是怎么做到的呢？事后看来，原理似乎相当简单：他们将深度学习运用到强化学习领域，结果却超越了他们最疯狂的设想。在本章中，我们将首先解释强化学习是什么，以及它擅长于什么，然后我们将介绍两个在深度强化学习领域最重要的技术：策略梯度和深度Q网络（DQN），包括讨论马尔可夫决策过程（MDP）。我们将使用这些技术来训练一个模型来平衡移动车上的杆子，另一个玩Atari游戏。同样的技术可以用于各种各样的任务，从步行机器人到自动驾驶汽车。

## 学习优化奖励

在强化学习中，一个点在环境中观察并且做出决策，随后它会得到奖励。它的目标是去学习如何行动能最大化期望的奖励。如果你不在意去拟人化的话，你可以认为正奖励是愉快，负奖励是痛苦（这样的话奖励一词就有点误导了）。简单来说，这个点在环境中行动，并且在实验和错误中去学习最大化它的愉快，最小化它的痛苦。

这是一个相当广泛的设置，可以适用于各种各样的任务。以下是几个例子（详见图16-1）：

a.这个点可以是控制一个机械狗的程序。在此例中，环境就是真实的世界，这个点通过许多的传感器例如摄像机或者传感器来观察，它可以通过给电机驱动信号来行动。它可以被编程设置为如果到达了目的地就得到正奖励，如果浪费时间，或者走错方向，或摔倒了就得到负奖励。
b.这个点可以是控制MS.Pac-Man的程序。在此例中，环境是Atari游戏的仿真，行为是9个操纵杆位（上下左右中间等等），观察是屏幕，回报就是游戏点数。
c.相似地，这个点也可以是棋盘游戏的程序例如：GO游戏。
d.
e.

![图16-1](../images/chapter_16/16-1.png)






