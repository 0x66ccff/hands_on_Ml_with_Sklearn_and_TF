<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 第8章 降维

很多机器学习的问题都会涉及到有着几千甚至数百万维的特征的训练实例。这不仅让训练过程变得非常缓慢，同时还很难找到一个很好的解，我们接下来就会遇到这种情况。这种问题通常被称为维数灾难(curse of dimentionality)。  

幸运的是，在现实生活中我们经常可以极大的降低特征维度，将一个十分棘手的问题转变成一个可以较为容易解决的问题。例如，对于MNIST图片集（第3章中提到）：图片四周边缘部分的像素几乎总是白的，因此你完全可以将这些像素从你的训练集中扔掉而不会丢失太多信息。图7-6向我们证实了这些像素的确对我们的分类任务是完全不重要的。同时，两个相邻的像素往往是高度相关的：如果你想要将他们合并成一个像素（比如取这两个像素点的平均值）你并不会丢失很多信息。  

> 警告：降维肯定会丢失一些信息（这就好比将一个图片压缩成JPEG的格式会降低图像的质量），因此即使这种方法可以加快训练的速度，同时也会让你的系统表现的稍微差一点。降维会让你的工作流水线更复杂因而更难维护。所有你应该先尝试使用原始的数据来训练，如果训练速度太慢的话再考虑使用降维。在某些情况下，降低训练集数据的维度可能会筛选掉一些噪音和不必要的细节，这可能会让你的结果比降维之前更好（这种情况通常不会发生；它只会加快你训练的速度）。

降维除了可以加快训练速度外，在数据可视化方面（或者DataViz)也十分有用。降低特征维度到2（或者3）维从而可以在图中画出一个高维度的训练集，让我们可以通过视觉直观的发现一些非常重要的信息，比如聚类。  

在这一章里，我们将会讨论维数灾难问题并且了解<b>在高维空间的数据</b>。然后，我们将会展示两种主要的降维方法：映射（projection）和流形学习（Manifold Learning），同时我们还会介绍三种流行的降维技术：主成分分析（PCA），核主成分分析（Kernel PCA）和局部线性嵌入（LLE）。  

## 维数灾难

我们已经习惯生活在一个三维的世界里，以至于当我们尝试想象更高维的空间时，我们的直觉不管用了。即使是一个基本的4D超正方体也很难在我们的脑中想象出来（见图8-1），更不用说一个200维的椭球弯曲在一个1000维的空间里了。  

![](Figure 8-1)
<caption><i>图8-1 点，线，方形，立方体和超正方体（0D到4D超正方体）</i></caption>  

这表明很多物体在高维空间表现的十分不同。比如，如果你在一个正方形单元中随机取一个点（一个1×1的正方形），那么随机选的点离所有边界大于0.001（靠近中间位置）的概率为0.4%（1 - 0.998<sup>2</sup>）（换句话说，一个随机产生的点不大可能严格落在摸一个<b>维度上</b>。但是在一个1,0000维单元的超正方体（一个1×1×...×1的立方体，有10,000个1），这种可能性超过了99.999999%。在高维超正方体中，大多数点都分布在边界处。  

**APPENDIX HERE**  

还有一个更麻烦的区别：如果你在一个平方单位中随机选取两个点，那么这两个点之间的距离平均约为0.52。如果您在单位3D立方体中选取两个随机点，平均距离将大致为0.66。但是，在一个1,000,000维超立方体中随机抽取两点呢？那么，平均距离，信不信由你，大概为408.25（大致
√1,000,000 / 6）！<b>!!!!!根号</b>这非常违反直觉：当它们都位于同一单元超立方体内时，两点是怎么距离这么远的？这一事实意味着高维数据集有很大风险分布的非常稀疏：大多数训练实例可能彼此远离。当然，这也意味着一个新实例可能远离任何训练实例，这使得预测的可靠性远低于我们处理较低维度数据的预测，因为它们将基于更大的推测（extrapolations)。简而言之，训练集的维度越高，过拟合的风险就越大。  

理论上来说，维数爆炸的一个解决方案是增加训练集的大小从而达到拥有足够密度的训练集。不幸的是，在实践中，达到给定密度所需的训练实例的数量随着维度的数量呈指数增长。如果只有100个特征（比MNIST问题要少得多）并且假设它们均匀分布在所有维度上，<b>那么如果想要各个临近的训练实例之间的距离在0.1以内</b>，您需要比宇宙中的原子还要多的训练实例。  

## 降维的主要方法

在我们深入研究具体的降维算法之前，我们来看看降低维度的两种主要方法：投影和流形学习。  

### 投影（Projection）

在大多数现实生活的问题中，训练实例并不是在所有维度上均匀分布的。许多特征几乎是常数，而其他特征则高度相关（如前面讨论的MNIST）。结果，所有训练实例实际上位于（或接近）高维空间的低维子空间内。这听起来有些抽象，所以我们不妨来看一个例子。在图8-2中，您可以看到由圆圈表示的3D数据集。  

![]()

<caption><i>图8-2 一个分布接近于2D子空间的3D数据集</i></caption>

注意到所有训练实例的分布都贴近一个平面：这是高维（3D）空间的较低维（2D）子空间。现在，如果我们将每个训练实例垂直投影到这个子空间上（就像将短线连接到平面的点所表示的那样），我们就可以得到如图8-3所示的新2D数据集。铛铛铛！我们刚刚将数据集的维度从3D降低到了2D。请注意，坐标轴对应于新的特征z<sub>1</sub>和z<sub>2</sub>（平面上投影的坐标）。  

![]()

<caption><i>一个经过投影后的新的2D数据集</i></caption>  

但是，投影并不总是降维的最佳方法。在很多情况下，子空间可能会扭曲和转动，比如图8-4所示的着名瑞士滚动玩具数据集。  

![]()

<caption><i>瑞士滚动数玩具数据集</i></caption>  

简单地将数据集投射到一个平面上（例如，直接丢弃x<sub>3</sub>）会将瑞士卷的不同层叠在一起，如图8-5左侧所示。但是，你真正想要的是展开瑞士卷所获取到的类似图8-5右侧的2D数据集。  

![]()

<caption><i>图8-5 投射到平面的压缩（左）vs 展开瑞士卷（右）</i></caption>

瑞士卷一个是二维流形的例子。简而言之，二维流形是一种二维形状，它可以在更高维空间中弯曲或扭曲。更一般地，<b>一个d维流形是类似于d维超平面的n维空间</b>（其中d < n）的一部分。在我们瑞士卷这个例子中，d = 2，n = 3：它有些像2D平面，但是它实际上是在第三维中卷曲。  

许多降维算法通过对训练实例所在的流形进行建模从而达到降维目的；这叫做流形学习。它依赖于流形猜想（manifold assumption），也被称为流形假设（manifold hypothesis），它认为大多数现实世界的高维数据集大都靠近一个更低维的流形。<b>这种假设经常在实践中被观察到。</b>  

让我们再回到MNIST数据集：所有手写数字图像都有一些相似之处。它们由连线组成，边界是白色的，大多是在图片中中间的，等等。如果你随机生成图像，只有一小部分看起来像手写数字。换句话说，如果您尝试创建数字图像，那么您的自由度远低于您生成任何随便一个图像时的自由度。这些约束往往会将数据集压缩到较低维流形中。  

流形假设通常包含着另一个隐含的假设：你现在的手上的工作（例如分类或回归）如果在流形的较低维空间中表示，那么它们会变得更简单。例如，在图8-6的第一行中，瑞士卷被分为两类：在三维空间中（图左上），分类边界会相当复杂，但在二维展开的流形空间中（图右上），分类边界是一条简单的直线。  

但是，这个假设并不总是成立。例如，在图8-6的最下面一行，决策边界位于x<sub>1</sub> = 5（图左下）。这个决策边界在原始三维空间（一个垂直平面）看起来非常简单，但在展开的流形中却变得更复杂了（四个独立线段的集合）（图右下）。  

简而言之，如果在训练模型之前降低训练集的维数，那训练速度肯定会加快，但并不总是会得出更好的训练效果；这一切都取决于数据集。
希望你现在对于维数爆炸以及降维算法如何解决这个问题有了一定的理解，<b>特别是对流形假设的提出</b>。本章的其余部分将介绍一些最流行的降维算法。  

![]()

<caption><i>图 8-6 决策边界并不总是会在低维空间中变的简单</i></caption>  

## 主成分分析（PCA）

主成分分析（Principal Component Analysis）是目前为止最流行的降维算法。首先它找到接近数据集分布的超平面，然后将所有的数据都投影到这个超平面上。  

### 保留（最大）方差

在将训练集投影到较低维超平面之前，您首先需要选择正确的超平面。例如图8-7左侧是一个简单的二维数据集，以及三个不同的轴（即一维超平面）。图右边是将数据集投影到每个轴上的结果。正如你所看到的，投影到实线上保留了最大方差，而在点线上的投影只保留了非常小的方差，投影到虚线上保留的方差则处于上述两者之间。  

![]()

<caption><i>图 8-7 选择投射到哪一个子空间</i></caption>  

选择保持最大方差的轴看起来是合理的，因为它很可能比其他投影损失更少的信息。证明这种选择的另一种方法是，选择这个轴使得将原始数据集投影到该轴上的均方距离最小。这是就PCA背后的思想，相当简单。  

### 主成分（Principle Componets）

PCA寻找训练集中可获得最大方差的轴。在图8-7中，它是一条实线。它还发现了一个与第一个轴正交的第二个轴，选择它可以获得最大的残差。在这个2D例子中，没有选择：就只有这条点线。但如果在一个更高维的数据集中，PCA也可以找到与前两个轴正交的第三个轴，以及与数据集中维数相同的第四个轴，第五个轴等。
定义第i个轴的单位矢量被称为第i个主成分（PC）。在图8-7中，第一个PC是**c<sub>1</sub>**，第二个PC是**c<sub>2</sub>**。在图8-2中，前两个PC用平面中的正交箭头表示，第三个PC与上述PC形成的平面正交（指向上或下）。  

> 概述： 主成分的方向不稳定：如果您稍微打乱一下训练集并再次运行PCA，则某些新PC可能会指向与原始PC方向相反。但是，它们通常仍位于同一轴线上。在某些情况下，一对PC甚至可能会旋转或交换，但它们定义的平面通常保持不变。  

那么如何找到训练集的主成分呢？幸运的是，有一种称为奇异值分解（SVD）的标准矩阵分解技术，可以将训练集矩阵X分解为三个矩阵U·Σ·V<sup>T</sup>的点积，其中V<sup>T</sup>包含我们想要的所有主成分，如公式8-1所示。  

<caption><i>公式 8-1 主成分矩阵</i></caption>

下面的Python代码使用了`Numpy`提供的`svd()`函数获得训练集的所有主成分，然后提取前两个PC:  

```Python
X_centered=X-X.mean(axis=0)
U,s,V=np.linalg.svd(X_centered)
c1=V.T[:,0]
c2=V.T[:,1]
```

> 警告：PCA假定数据集以原点为中心。正如我们将看到的，Scikit-Learn的PCA类负责为您的数据集中心化处理。但是，如果您自己实现PCA（如前面的示例所示），或者如果您使用其他库，不要忘记首先要先对数据做中心化处理。

### 投影到d维空间

一旦确定了所有的主成分，你就可以通过将数据集投影到由前d个主成分构成的超平面上，从而将数据集的维数降至d维。选择这个超平面可以确保投影将保留尽可能多的方差。例如，在图8-2中，3D数据集被投影到由前两个主成分定义的2D平面，保留了大部分数据集的方差。因此，2D投影看起来非常像原始3D数据集。  

为了将训练集投影到超平面上，可以简单地通过计算训练集矩阵X和W<sub>d</sub>的点积，W<sub>d</sub>定义为包含前d个主成分的矩阵（即由V<sup>T</sup>的前d列组成的矩阵），如等式8-2所示。

<caption><i>等式 8-2 将训练集投影到d维空间</i></caption>

X<sub>d-proj</sub> = X · W<sub>d</sub>

下面的Python代码将训练集投影到由前两个主成分定义的超平面上：  

```Python
W2=V.T[:,:2]
X2D=X_centered.dot(W2)
```

好了你已经知道这个东西了！你现在已经知道如何给任何一个数据集降维而又能尽可能的保留原数据集的方差了。  

### 使用Scikit-Learn

Scikit-Learn的PCA类使用SVD分解来实现，就像我们之前做的那样。以下代码应用PCA将数据集的维度降至两维（请注意，它会自动处理数据的中心化）：  

```Python
from sklearn.decomposition import PCA

pca=PCA(n_components=2)
X2D=pca.fit_transform(X)
```

将PCA转化器应用于数据集后，可以使用`components_`访问每一个主成分（注意，它返回以PC作为水平向量的矩阵，因此，如果我们想要获得第一个主成分则可以写成`pca.components_.T[:,0]`）。

### 方差解释率（Explained Variance Ratio）

另一个非常有用的信息是每个主成分的方差解释率，可通过`explained_variance_ratio_`变量获得。它表示位于每个主成分轴上的数据集方差的比例。例如，让我们看一下图8-2中表示的三维数据集前两个分量的方差解释率：  

```
>>> print(pca.explained_variance_ratio_)
array([0.84248607, 0.14631839])
```

这表明，84.2％的数据集方差位于第一轴，14.6％的方差位于第二轴。第三轴的这一比例不到1.2％，因此可以认为它可能没有包含什么信息。  

### 选择正确的维度

通常我们倾向于选择加起来到方差解释率能够达到足够占比（例如95％）的维度的数量，而不是任意选择要降低到的维度数量。当然，除非您正在为数据可视化而降低维度--在这种情况下，您通常希望将维度降低到2或3。  

下面的代码在不降维的情况下进行PCA，然后计算出保留训练集方差95％所需的最小维数：  

```Python
pca=PCA()
pac.fit(X)
cumsum=np.cumsum(pca.explained_variance_ratio_)
d=np.argmax(cumsum>=0.95)+1
```

你可以设置`n_components = d`并再次运行PCA。但是，有一个更好的选择：不指定你想要保留的主成分个数，而是将`n_components`设置为0.0到1.0之间的浮点数，表明您希望保留的方差比率：  

```Python
pca=PCA(n_components=0.95)
X_reduced=pca.fit_transform(X)
```

另一种选择是画出方差解释率关于维数的函数（简单地绘制`cumsum`；参见图8-8）。曲线中通常会有一个肘部，方差解释率停止快速增长。您可以将其视为数据集的真正的维度。在这种情况下，您可以看到将维度降低到大约100个维度不会失去太多的可解释方差。  

![]()

<caption><i>图 8-8 可解释方差关于维数的函数</i></caption>  

### PCA压缩

显然，在降维之后，训练集占用的空间要少得多。例如，尝试将PCA应用于MNIST数据集，同时保留95％的方差。你应该发现每个实例只有150多个特征，而不是原来的784个特征。因此，尽管大部分方差都保留下来，但数据集现在还不到其原始大小的20％！这是一个合理的压缩比率，您可以看到这可以如何极大地加快分类算法（如SVM分类器）的速度。  

通过应用PCA投影的逆变换，也可以将缩小的数据集解压缩回784维。当然这并不会返回给你最原始的数据，因为投影丢失了一些信息（在5％的方差内），但它可能非常接近原始数据。原始数据和重构数据之间的均方距离（压缩然后解压缩）被称为重构误差（reconstruction error）。例如，下面的代码将MNIST数据集压缩到154维，然后使用`inverse_transform()`方法将其解压缩回784维。图8-9显示了原始训练集（左侧）的几位数字在压缩并解压缩后（右侧）的对应数字。您可以看到有轻微的图像质量降低，但数字仍然大部分完好无损。  

```Python
pca=PCA(n_components=154)
X_mnist_reduced=pca.fit_transform(X_mnist)
X_mnist_recovered=pca.inverse_transform(X_mnist_reduced)
```

![]()

<caption><i>图 8-9 MNIST保留95方差的压缩</i></caption>  

逆变换的公式如等式8-3所示  

<caption><i>等式 8-3 PCA逆变换，回退到原来的数据维度</i></caption>  

X<sub>recovered</sub> = X<sub>d-proj</sub> · W<sub>d</sub><sup>T</sup>  

### 增量PCA（Incremental PCA）

先前PCA实现的一个问题是它需要在内存中处理整个训练集以便SVD算法运行。幸运的是，我们已经开发了增量PCA（IPCA）算法：您可以将训练集分批，并一次只对一个批量使用IPCA算法。这对大型训练集非常有用，并且可以在线应用PCA（即在新实例到达时即时运行）。  
下面的代码将MNIST数据集分成100个小批量（使用NumPy的`array_split()`函数），并将它们提供给Scikit-Learn的`IncrementalPCA`类，以将MNIST数据集的维度降低到154维（就像以前一样）。请注意，您必须对每个最小批次调用`partial_fit()`方法，而不是对整个训练集使用`fit()`方法：  
```Python
from sklearn.decomposition import IncrementalPCA

n_batches=100
inc_pca=IncrementalPCA(n_components=154)
for X_batch in np.array_spplit(X_mnist,n_batches):
	inc_pca.partial_fit(X_batch)
X_mnist_reduced=inc_pca.transform(X_mnist)
```

或者，您可以使用NumPy的`memmap`类，它允许您操作存储在磁盘上二进制文件中的大型数组，就好像它完全在内存中;该类仅在需要时加载内存中所需的数据。由于增量PCA类在任何时间内仅使用数组的一小部分，因此内存使用量仍受到控制。这可以调用通常的`fit()`方法，如下面的代码所示：  

```Python
X_mm=np.memmap(filename,dtype='float32',mode='readonly',shape=(m,n))
batch_size=m//n_batches
inc_pca=IncrementalPCA(n_components=154,batch_size=batch_size)
inc_pca.fit(X_mm)
```

### 随机PCA（Randomized PCA）

Scikit-Learn提供了另一种执行PCA的选择，称为随机PCA。这是一种随机算法，可以快速找到前d个主成分的近似值。它的计算复杂度是O ( m × d<sup>2</sup> ) + O ( d<sup>3</sup> )，而不是O ( m × n<sup>2</sup> ) + O ( n<sup>3</sup> )，所以当d远小于n时，它比之前的算法快得多。  

```Python
rnd_pca=PCA(n_components=154,svd_solver='randomized')
X_reduced=rnd_pca.fit_transform(X_mnist)
```

## 核PCA（Kernel PCA）

在第5章中，我们讨论了核技巧，一种将实例隐式映射到非常高维空间（称为特征空间）的数学技术，让支持向量机可以应用于非线性分类和回归。回想一下，高维特征空间中的线性决策边界对应于原始空间中的复杂非线性决策边界。  

事实证明，同样的技巧可以应用于PCA，从而可以执行复杂的非线性投影来降低维度。这就是所谓的核PCA（kPCA）。它通常能够很好地保留投影后的簇，有时甚至可以展开分布近似于扭曲流形的数据集。  

例如，下面的代码使用Scikit-Learn的`KernelPCA`类来执行带有RBF核的kPCA（有关RBF内核和其他核的更多详细信息，请参阅第5章）：

```Python
from sklearn.decomposition import KernelPCA

rbf_pca=KernelPCA(n_components=2,kernel='rbf',gamma=0.04)
X_reduced=rbf_pca.fit_transform(X)
```

图8-10展示了使用线性核（等同于简单的使用PCA类），RBF核，sigmoid核<b>(逻辑)</b>对瑞士卷降到2维。

![]()

<caption><i>图 8-10 使用不同核的kPCA将瑞士卷降到2维</i></caption>






