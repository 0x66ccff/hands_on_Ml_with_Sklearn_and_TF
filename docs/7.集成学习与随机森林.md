# 第7章 集成学习与随机森林

假设你去随机问很多人一个很复杂的问题，然后把它们的答案合并起来。通常情况下你会发现这个合并的答案比一个专家的答案要好。这就叫做 *群体智慧* 。同样的，如果你合并了一组分类器的预测（像分类或者回归），你也会得到一个比单一分类器更好的预测结果。这一组分类器就叫做集成；因此，这个技术就叫做集成学习，一个集成学习算法就叫做集成方法。

例如，你可以训练一组决策树分类器，每一个都在一个随机的训练集上。为了去做预测，你必须得到所有单一树的预测值，然后通过投票（例如第六章的练习）来预测类别。例如一种决策树的集成就叫做随机森林，它除了简单之外也是现今存在的最强大的机器学习算法之一。

向我们在第二章讨论的一样，我们会在一个项目快结束的时候使用集成算法，一旦你建立了一些好的分类器，就把他们合并为一个更好的分类器。事实上，在机器学习竞赛中获得胜利的接活经常会包含一些集成方法。

在本章中我们会讨论一下特别著名的集成方法，包括 *bagging, boosting, stacking* ，和其他一些算法。我们也会讨论随机森林。

## 投票分类

假设你已经训练了一些分类器，每一个都有80%的准确率。你可能有了一个逻辑斯蒂回归、或一个SVM、或一个随机森林，或者一个KNN，或许还有更多（详见图7-1）

![图7-1](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/Lisanaaa/images/chapter_3/chapter3.2.jpeg)（未改）

一个非常简单去创建一个更好的分类器的方法就是去整合每一个分类器的预测然后经过投票去预测分类。这种分类器就叫做硬投票分类器（详见图7-2）。

![图7-2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/Lisanaaa/images/chapter_3/chapter3.2.jpeg)（未改）

令人惊奇的是这种投票分类器得出的结果经常会比集成中最好的一个分类器结果更好。事实上，即使每一个分类器都是一个弱学习器（意味着它们也就比瞎猜好点），集成后仍然是一个强学习器（高准确率），只要有足够数量的弱学习者，他们就足够多样化。








